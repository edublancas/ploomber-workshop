{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d748b4",
   "metadata": {},
   "source": [
    "# Ploomber Workshop Material\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Notebooks are an excellent environment for data exploration: they allow us to write code interactively and get visual feedback, providing an unbeatable experience for understanding our data.\n",
    "\n",
    "However, this convenience comes at a cost: if we are not careful about adding and removing code cells, we may have an irreproducible notebook. Arbitrary execution order is a prevalent problem: A [recent analysis](https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/) found that about 36% of notebooks on GitHub did not execute in linear order. To ensure our notebooks run, we must continuously test them to catch these problems.\n",
    "\n",
    "A second notable problem is the size of notebooks: the more cells we have, the more difficult it is to debug since there are more variables and code involved.\n",
    "\n",
    "Software engineers typically break down projects into multiple steps and test continuously to prevent broken and unmaintainable code. However, applying these ideas for data analysis requires extra work: multiple notebooks imply we have to ensure the output from one stage becomes the input for the next one. Furthermore, we can no longer press \"Run all cells\" in Jupyter to test our analysis from start to finish.\n",
    "\n",
    "**Ploomber provides all the necessary tools to build multi-stage, reproducible pipelines in Jupyter that feel like a single notebook.** Users can easily break down their analysis into multiple notebooks and execute them all with a single command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d725ce",
   "metadata": {},
   "source": [
    "## 2. Refactoring a legacy notebook\n",
    "\n",
    "If you already have a project in a single notebook, you can use our tool [Soorgeon](https://github.com/ploomber/soorgeon) to automatically refactor it into a [Ploomber](https://github.com/ploomber/ploomber) pipeline.\n",
    "\n",
    "Let's use the sample notebook in the `playground/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388facc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e647c",
   "metadata": {},
   "source": [
    "Our sample notebook is the [`nb.ipynb`](playground/nb.ipynb) file, let's take a look at it.\n",
    "\n",
    "To refactor the notebook, we use the `soorgeon refactor` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c42d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "soorgeon refactor nb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166114f",
   "metadata": {},
   "source": [
    "Let's take a look at the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5f473",
   "metadata": {},
   "source": [
    "We can see that we have a few new files. `pipeline.yaml` contains the pipeline declaration, and `tasks/` contains the *stages* that Soorgeon identified based on our H2 Markdown headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36248e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls playground/tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca99bd1",
   "metadata": {},
   "source": [
    "Let's plot the pipeline (note that we're now using `ploomber`, which is the framework for developing pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d841d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a31f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('playground/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110adf7",
   "metadata": {},
   "source": [
    "Soorgeon correctly identified the *stages* in our original `nb.ipynb` notebook. It even detected that the last two tasks (`linear-regression`, and `random-forest-regressor` are independenf of each other!).\n",
    "\n",
    "We can also get a summary of the pipeline with `ploomber status`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b54a06",
   "metadata": {},
   "source": [
    "## 3. The `pipeline.yaml` file\n",
    "\n",
    "To develop a pipeline, users create a `pipeline.yaml` file and declare the tasks and their outputs as follows:\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - source: script.py\n",
    "    product:\n",
    "      nb: output/executed.ipynb\n",
    "      data: output/data.csv\n",
    "  \n",
    "  # more tasks here...\n",
    "```\n",
    "\n",
    "The previous pipeline has a single task (`script.py`) and generates two outputs: `output/executed.ipynb` and `output/data.csv`. You may be wondering why we have a notebook as an output: Ploomber converts scripts to notebooks before execution; hence, our script is considered the source and the notebook a byproduct of the execution. Using scripts as sources (instead of notebooks) makes it simpler to use git. However, this does not mean you have to give up interactive development since Ploomber integrates with Jupyter, allowing you to edit scripts as notebooks.\n",
    "\n",
    "\n",
    "In this case, since we used `soorgeon` to refactor an existing notebook, we didn't have to write the `pipeline.yaml` file, let's take a look at the auto-generated one: [`playground/pipeline.yaml`](playground/pipeline.yaml).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05118517",
   "metadata": {},
   "source": [
    "## 4. Building the pipeline\n",
    "\n",
    "Let's build the pipeline (this will take ~30 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b535df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93feba",
   "metadata": {},
   "source": [
    "Navigate to `playground/output/` and you'll see all the outputs: the executed notebooks, data files and trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls playground/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb50947",
   "metadata": {},
   "source": [
    "## 5. Declaring dependencies\n",
    "\n",
    "Let's look again at our pipeline plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821241da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('playground/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacbbfd",
   "metadata": {},
   "source": [
    "The arrows in the diagram represent input/output dependencies, hence, determine execution order. For example, the first task (`load`) loads some data, then `clean` uses such data as input and process it, then `train-test-split` splits our dataset in training and test, finally, we use those datasets to train a linear regression and a random forest regressor.\n",
    "\n",
    "Soorgeon extracted and declared this dependencies for us, but if we want to modify the existing pipeline, we need to declare such dependencies. Let's see how.\n",
    "\n",
    "\n",
    "## 6. Adding a new task\n",
    "\n",
    "Let's say we want to train another model and decide to try [Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor). First, we modify the `pipeline.yaml` file and add a new task:\n",
    "\n",
    "####  Open `playground/pipeline.yaml` and add the following lines at the end\n",
    "\n",
    "```yaml\n",
    "- source: tasks/gradient-boosting-regressor.py\n",
    "  product:\n",
    "    nb: output/gradient-boosting-regressor.ipynb\n",
    "```\n",
    "\n",
    "Now, let's create a base file by executing `ploomber scaffold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcc565",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber scaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb55c7",
   "metadata": {},
   "source": [
    "Let's see how the plot looks now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efe6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f4a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('playground/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8031df5",
   "metadata": {},
   "source": [
    "You can see that Ploomber recognizes the new file, but it doesn't have any dependency, so let's tell Ploomber that it should execute after `train-test-split`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897254d",
   "metadata": {},
   "source": [
    "####  Open `playground/tasks/gradient-boosting-regressor.py` as a notebook by right-clicking on it and then `Open With` -> `Notebook`:\n",
    "\n",
    "![lab-open-with-notebook](images/lab-open-with-notebook.png)\n",
    "\n",
    "At the top of the notebook, you'll see the following:\n",
    "\n",
    "```python\n",
    "upstream = None\n",
    "```\n",
    "\n",
    "This special variable indicates which tasks should execute before the notebook we're currently working on. In this case, we want to get training data so we can train our new model so we change the `upstream` variable:\n",
    "\n",
    "```python\n",
    "upstream = ['train-test-split']\n",
    "```\n",
    "\n",
    "Let's generate the plot again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('playground/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3461d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Ploomber now recognizes our dependency declaration!\n",
    "\n",
    "####  Open `playground/tasks/gradient-boosting-regressor.py` as a notebook by right-clicking on it and then `Open With` -> `Notebook` and add the following code:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "y_train = pickle.loads(Path(upstream['train-test-split']['y_train']).read_bytes())\n",
    "y_test = pickle.loads(Path(upstream['train-test-split']['y_test']).read_bytes())\n",
    "X_test = pickle.loads(Path(upstream['train-test-split']['X_test']).read_bytes())\n",
    "X_train = pickle.loads(Path(upstream['train-test-split']['X_train']).read_bytes())\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90662f",
   "metadata": {},
   "source": [
    "## 7. Incremental builds\n",
    "\n",
    "Data workflows require a lot of iteration. For example, you may want to generate a new feature or model. However, it's wasteful to re-execute every task with every minor change. Therefore, one of Ploomber's core features is incremental builds, which automatically skip tasks whose source code hasn't changed.\n",
    "\n",
    "Run the pipeline again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e629caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63e01e",
   "metadata": {},
   "source": [
    "You can see that only the `gradient-boosting-regressor` task ran!\n",
    "\n",
    "Incremental builds allow us to iterate faster without keeping track of task changes.\n",
    "\n",
    "Check out [`playground/output/gradient-boosting-regressor.ipynb`](playground/output/gradient-boosting-regressor.ipynb), which contains the output notebooks with the model evaluation plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525e9f6",
   "metadata": {},
   "source": [
    "## 8. Execution in the cloud\n",
    "\n",
    "When working with datasets that fit in memory, running your pipeline is simple enough, but sometimes you may need more computing power for your analysis. Ploomber makes it simple to execute your code in a distributed environment without code changes.\n",
    "\n",
    "Check out [Soopervisor](https://soopervisor.readthedocs.io), the package that implements exporting Ploomber projects in the cloud with support for:\n",
    "\n",
    "* [Kubernetes (Argo Workflows)](https://soopervisor.readthedocs.io/en/latest/tutorials/kubernetes.html)\n",
    "* [AWS Batch](https://soopervisor.readthedocs.io/en/latest/tutorials/aws-batch.html)\n",
    "* [Airflow](https://soopervisor.readthedocs.io/en/latest/tutorials/airflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f2085",
   "metadata": {},
   "source": [
    "# 9. Resources\n",
    "\n",
    "Thanks for taking the time to go through this tutorial! We hope you consider using Ploomber for your next project. If you have any questions or need help, please reach out to us! (contact info below).\n",
    "\n",
    "Here are a few resources to dig deeper:\n",
    "\n",
    "* [GitHub](https://github.com/ploomber/ploomber)\n",
    "* [Documentation](https://ploomber.readthedocs.io/)\n",
    "* [Code examples](https://github.com/ploomber/projects)\n",
    "* [JupyterCon 2020 talk](https://www.youtube.com/watch?v=M6mtgPfsA3M)\n",
    "* [Argo Community Meeting talk](https://youtu.be/FnpXyg-5W_c)\n",
    "* [Pangeo Showcase talk (AWS Batch demo)](https://youtu.be/XCgX1AszVF4)\n",
    "\n",
    "# 10. Contact\n",
    "\n",
    "* Twitter: [@ploomber](https://twitter.com/ploomber)\n",
    "* Join us on Slack: [http://ploomber.io/community](http://ploomber.io/community)\n",
    "* E-mail: [contact@ploomber.io](mailto:contact@ploomber.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

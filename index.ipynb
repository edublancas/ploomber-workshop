{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104f7ec3",
   "metadata": {},
   "source": [
    "# Ploomber Workshop Material\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Notebooks are an excellent environment for data exploration: they allow us to write code interactively and get visual feedback, providing an unbeatable experience for understanding our data.\n",
    "\n",
    "However, this convenience comes at a cost: if we are not careful about adding and removing code cells, we may have an irreproducible notebook. Arbitrary execution order is a prevalent problem: A [recent analysis](https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/) found that about 36% of notebooks on GitHub did not execute in linear order. To ensure our notebooks run, we must continuously test them to catch these problems.\n",
    "\n",
    "A second notable problem is the size of notebooks: the more cells we have, the more difficult it is to debug since there are more variables and code involved.\n",
    "\n",
    "Software engineers typically break down projects into multiple steps and test continuously to prevent broken and unmaintainable code. However, applying these ideas for data analysis requires extra work: multiple notebooks imply we have to ensure the output from one stage becomes the input for the next one. Furthermore, we can no longer press \"Run all cells\" in Jupyter to test our analysis from start to finish.\n",
    "\n",
    "**Ploomber provides all the necessary tools to build multi-stage, reproducible pipelines in Jupyter that feel like a single notebook.** Users can easily break down their analysis into multiple notebooks and execute them all with a single command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5de4f6",
   "metadata": {},
   "source": [
    "## Running a pipeline\n",
    "\n",
    "Pipelines are composed of multiple files; the `example/` directory contains a complete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0849542",
   "metadata": {},
   "source": [
    "We can see the following:\n",
    "\n",
    "* `output/` - directory to save our output (won't exist until after the first run)\n",
    "* `pipeline.yaml` - pipeline declaration\n",
    "* `scripts/` - our source code\n",
    "\n",
    "Note that even though we have `.py` files, we'll open them as notebooks. The following section explains why and how.\n",
    "\n",
    "Let's look at the scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce42d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls example/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c63529",
   "metadata": {},
   "source": [
    "There are four of them; each file corresponds to a single task in our pipeline.\n",
    "\n",
    "Let's run the pipeline:\n",
    "\n",
    "**Note:** This takes about 10 seconds to run; you won't see any output until it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd example\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae447a34",
   "metadata": {},
   "source": [
    "### That's it! You just ran your first Ploomber pipeline!\n",
    "\n",
    "Let's now see how the different pieces come together by re-creating the example pipeline from scratch.\n",
    "\n",
    "## Creating a pipeline in Ploomber\n",
    "\n",
    "To develop a pipeline, users create a `pipeline.yaml` file and declare the tasks and their outputs as follows:\n",
    "\n",
    "```yaml\n",
    "tasks:\n",
    "  - source: script.py\n",
    "    product:\n",
    "      nb: output/executed.ipynb\n",
    "      data: output/data.csv\n",
    "  \n",
    "  # more tasks here...\n",
    "```\n",
    "\n",
    "The previous pipeline has a single task (`script.py`) and generates two outputs: `output/executed.ipynb` and `output/data.csv`. You may be wondering why we have a notebook as an output: Ploomber converts scripts to notebooks before execution; hence, our script is considered the source and the notebook a byproduct of the execution. The use of scripts as sources (instead of notebooks) makes it simpler to use git. However, this does not mean you have to give up interactive development since Ploomber integrates with Jupyter, allowing you to edit scripts as notebooks.\n",
    "\n",
    "**Let's now build a simple four-step pipeline**:\n",
    "\n",
    "```yaml\n",
    "# copy this YAML into playground/pipeline.yaml\n",
    "tasks:\n",
    "  - source: scripts/get.py\n",
    "    product:\n",
    "      nb: output/get.ipynb\n",
    "      data: output/get.csv\n",
    "\n",
    "  - source: scripts/feature-sepal.py\n",
    "    product:\n",
    "      nb: output/feature-sepal.ipynb\n",
    "      data: output/feature-sepal.csv\n",
    "\n",
    "  - source: scripts/feature-petal.py\n",
    "    product:\n",
    "      nb: output/feature-petal.ipynb\n",
    "      data: output/feature-petal.csv\n",
    "\n",
    "  - source: scripts/fit.py\n",
    "    product:\n",
    "      nb: output/fit.ipynb\n",
    "      model: output/model.pickle\n",
    "```\n",
    "\n",
    "The pipeline above has four tasks: one that obtains raw data (the iris dataset), two that process such data, and a final task that trains a model.\n",
    "\n",
    "#### Step 1: Copy the snippet contents above into `playground/pipeline.yaml` (Create the file first, save before running the next cell).\n",
    "\n",
    "Our source code is still missing; let's ask Ploomber to generate some base files for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber scaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373f864",
   "metadata": {},
   "source": [
    "Let's now ensure that our pipeline is appropriately recognized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5323006",
   "metadata": {},
   "source": [
    "We can see that Ploomber recognizes our pipeline and prints its status. Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29827d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb49970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('playground/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b833eb",
   "metadata": {},
   "source": [
    "\n",
    "**Our pipeline doesn't have any structure yet, but we can easily add it.**\n",
    "\n",
    "We want to:\n",
    "\n",
    "1. Get data\n",
    "2. Process it\n",
    "3. Fit a model\n",
    "\n",
    "Let's edit those files.\n",
    "\n",
    "#### Step 2: Open `playground/scripts/features-petal.py` as a notebook by right-clicking on it and then `Open With` -> `Notebook`:\n",
    "\n",
    "![lab-open-with-notebook](images/lab-open-with-notebook.png)\n",
    "\n",
    "At the top of the notebook, you'll see the following:\n",
    "\n",
    "```python\n",
    "upstream = None\n",
    "```\n",
    "\n",
    "This special variable indicates which tasks should execute before the notebook we're currently working on. In this case, we want to get the raw data so we can process it here, so we do:\n",
    "\n",
    "```python\n",
    "upstream = ['get']\n",
    "```\n",
    "\n",
    "#### Step 3: In `playground/scripts/features-petal.py`, replace `upstream = None` with `upstream = ['get']`\n",
    "\n",
    "So far, we've told ploomber that it should run `get.py` before `features-petal.py`. But we cannot execute this interactively, since we don't know where `get.py` is saving its output, but we can quickly fix that.\n",
    "\n",
    "#### Step 4: In `playground/scripts/features-petal.py`, save the file, then click on `File` -> `Reload Python File from Disk`:\n",
    "\n",
    "![reload-file](images/reload-file.png)\n",
    "\n",
    "You'll see that the script updates. Since you said you want to execute `get` first, Ploomber adds a new cell that contains the output files of `get`.\n",
    "\n",
    "**Ploomber makes it simple to assemble a data pipeline from multiple notebooks!**\n",
    "\n",
    "Let's continue declaring the remaining dependencies for the other tasks:\n",
    "\n",
    "**Note:** Remember to open the following two scripts as notebooks (right-clicking and then `Open With` -> `Notebook`)\n",
    "\n",
    "#### Step 5: Edit `playground/scripts/features-sepal.py`, change `upstream = None` to `upstream = ['get']`, and save\n",
    "#### Step 6: Edit `playground/scripts/fit.py`, change `upstream = None` to `upstream = ['get', 'feature-sepal', 'feature-petal']`, and save\n",
    "\n",
    "Let's now re-create the pipeline plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('playground/pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc106f93",
   "metadata": {},
   "source": [
    "\n",
    "Ploomber recognizes the references and draws the dependency relationship among tasks.\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "![pipeline-diagram](images/pipeline-diagram.png)\n",
    "\n",
    "Our pipeline doesn't do anything yet. So let's add some code. Edit the files and add the following:\n",
    "\n",
    "**Note:** make sure you add the code in a new cell **at the end** of the notebook.\n",
    "\n",
    "#### Step 7: Paste the following code in  `playground/scripts/get.py`\n",
    "\n",
    "```python\n",
    "# get raw data\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "raw = load_iris(as_frame=True)\n",
    "df = raw['data']\n",
    "df.head()\n",
    "df['target'] = raw['target']\n",
    "df.to_csv(product['data'], index=False)\n",
    "```\n",
    "\n",
    "#### Step 8: Paste the following code in `playground/scripts/feature-sepal.py`\n",
    "\n",
    "```python\n",
    "# generate one feature\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(upstream['get']['data'])\n",
    "df['sepal-area'] = df['sepal length (cm)'] * df['sepal width (cm)']\n",
    "df[['sepal-area']].to_csv(product['data'], index=False)\n",
    "```\n",
    "\n",
    "#### Step 9: Paste the following code in `playground/scripts/feature-petal.py`\n",
    "\n",
    "```python\n",
    "# generate another feature\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(upstream['get']['data'])\n",
    "df['petal-area'] = df['petal length (cm)'] * df['petal width (cm)']\n",
    "df[['petal-area']].to_csv(product['data'], index=False)\n",
    "```\n",
    "\n",
    "#### Step 10: Paste the following code in `playground/scripts/fit.py`\n",
    "\n",
    "```python\n",
    "# train a model and save it\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn_evaluation import plot\n",
    "\n",
    "get = pd.read_csv(upstream['get']['data'])\n",
    "petal = pd.read_csv(upstream['feature-sepal']['data'])\n",
    "sepal = pd.read_csv(upstream['feature-petal']['data'])\n",
    "df = get.join(petal).join(sepal)\n",
    "\n",
    "X = df.drop('target', axis='columns')\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "Path(product['model']).write_bytes(pickle.dumps(model))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plot.confusion_matrix(y_test, y_pred)\n",
    "```\n",
    "\n",
    "We can now run our pipeline:\n",
    "\n",
    "**Note:** This takes about 10 seconds to run; you won't see any output until it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc32a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b720e8",
   "metadata": {},
   "source": [
    "Navigate to `playground/output/` and you'll see all the outputs: the executed notebooks, data files and trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls playground/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34573422",
   "metadata": {},
   "source": [
    "\n",
    "## Incremental builds\n",
    "\n",
    "Data workflows require a lot of iteration. For example, you may want to generate a new feature or model. However, it's wasteful to re-execute every task with every minor change. Therefore, one of Ploomber's core features is incremental builds, which automatically skip tasks whose source code hasn't changed.\n",
    "\n",
    "#### Step 11: Modify `playground/scripts/fit.py`, by adding a summary table as a new cell at the end:\n",
    "\n",
    "```python\n",
    "# add this at the bottom of playground/scripts/fit.py\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "Run the pipeline again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd playground\n",
    "ploomber build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0dbdf",
   "metadata": {},
   "source": [
    "You can see that only the `fit` task ran!\n",
    "\n",
    "Incremental builds allow us to iterate faster without having to keep track of task changes.\n",
    "\n",
    "Check out `playground/output/fit.ipynb`, which contains the output notebooks with the model evaluation plot and table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829fd48",
   "metadata": {},
   "source": [
    "## Execution in the cloud\n",
    "\n",
    "When working with datasets that fit in memory, running your pipeline is simple enough, but sometimes you may need more computing power for your analysis. Ploomber makes it simple to execute your code in a distributed environment without code changes.\n",
    "\n",
    "Check out [Soopervisor](soopervisor.readthedocs.io), the package that implements exporting Ploomber projects in the cloud with support for:\n",
    "\n",
    "* [Kubernetes (Argo Workflows)](https://soopervisor.readthedocs.io/en/latest/tutorials/kubernetes.html)\n",
    "* [AWS Batch](https://soopervisor.readthedocs.io/en/latest/tutorials/aws-batch.html)\n",
    "* [Airflow](https://soopervisor.readthedocs.io/en/latest/tutorials/airflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e38feb",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "Thanks for taking the time to go through this tutorial! We hope you consider using Ploomber for your next project. If you have any questions or need help, please reach out to us! (contact info below).\n",
    "\n",
    "Here are a few resources to dig deeper:\n",
    "\n",
    "* [GitHub](https://github.com/ploomber/ploomber)\n",
    "* [Documentation](https://ploomber.readthedocs.io/)\n",
    "* [Code examples](https://github.com/ploomber/projects)\n",
    "* [JupyterCon 2020 talk](https://www.youtube.com/watch?v=M6mtgPfsA3M)\n",
    "* [Argo Community Meeting talk](https://youtu.be/FnpXyg-5W_c)\n",
    "* [Pangeo Showcase talk (AWS Batch demo)](https://youtu.be/XCgX1AszVF4)\n",
    "\n",
    "# Contact\n",
    "\n",
    "* Twitter: [@ploomber](https://twitter.com/ploomber)\n",
    "* Join us on Slack: [http://community.ploomber.io](http://community.ploomber.io)\n",
    "* E-mail: [contact@ploomber.io](mailto:contact@ploomber.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
